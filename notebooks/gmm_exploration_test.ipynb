{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D mixture toy demonstration of VSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as td\n",
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.acquisition.analytic import LogProbabilityOfImprovement\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.priors import GammaPrior\n",
    "from scipy.stats import multinomial, multivariate_normal\n",
    "from sklearn.metrics import balanced_accuracy_score, r2_score\n",
    "\n",
    "from vsd.acquisition import (\n",
    "    LogPIClassifierAcquisition,\n",
    "    VariationalSearchAcquisition\n",
    ")\n",
    "from vsd.generation import generate_candidates_iw\n",
    "from vsd.proposals import GaussianKDEProposal\n",
    "from vsd.cpe import ContinuousCPEModel, fit_cpe\n",
    "from vsd.labellers import AnnealedThreshold\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(seed=SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sim properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAINING = 50\n",
    "GRADIENT_SAMPLES = 1024\n",
    "N_CANDIDATES = 30\n",
    "N_ROUNDS = 10\n",
    "USE_CLASSIFIER = True  # GP or CPE\n",
    "INITIAL_STD = 6\n",
    "INITIAL_MEAN = [0, 0]\n",
    "THRESH = 1.5\n",
    "INCREASING_THRESH = True # Swtich between FL (false) and BBO (true)\n",
    "PERCENTILE = 0.5\n",
    "TEMPERATURE = 0.7\n",
    "CLASSIFIER_REG = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate data\n",
    "\n",
    "Simulate a \"fitness landscape\" from a mixture of Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1., 1, 1, 1, 1, 1, 1])\n",
    "w /= sum(w)\n",
    "mus = np.array([\n",
    "    [-1, 3],\n",
    "    [1, 2],\n",
    "    [0, 4],\n",
    "    [-3, -1.5],\n",
    "    [-3, -3.5],\n",
    "    [2, -2],\n",
    "    [-1, 0]\n",
    "])\n",
    "covs = np.array([np.eye(2) for _ in range(len(mus))])\n",
    "norms = np.array([multivariate_normal(mean=m, cov=S) for m, S in zip(mus, covs)])\n",
    "\n",
    "def mixture_px(X):\n",
    "    pX = np.array([n.pdf(X) for n in norms]).squeeze()\n",
    "    pX = (w @ pX)\n",
    "    return pX * 100\n",
    "\n",
    "def mixture_sample(size=1):\n",
    "    pw = multinomial(n=1, p=w)\n",
    "    z = pw.rvs(size=size).astype(bool)\n",
    "    X = []\n",
    "    for i in range(size):\n",
    "        x = norms[z[i]][0].rvs()\n",
    "        X.append(x)\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = td.Independent(td.Normal(\n",
    "    torch.tensor(INITIAL_MEAN, dtype=float),\n",
    "    torch.tensor(INITIAL_STD, dtype=float)),\n",
    "    1)\n",
    "\n",
    "X = prior.sample([N_TRAINING])\n",
    "X = X.float()\n",
    "\n",
    "ngrid = 300\n",
    "gbound = 10\n",
    "mX, mY= np.meshgrid(np.linspace(-gbound, gbound, ngrid),\n",
    "                    np.linspace(-gbound, gbound, ngrid))\n",
    "gX = np.vstack([mX.flatten(), mY.flatten()]).T\n",
    "gXT = torch.Tensor(gX)\n",
    "pX = mixture_px(gX)\n",
    "pmax = pX.max()\n",
    "\n",
    "def plot_contours(f, Xsamples=None, ax=None, title=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(dpi=150, figsize=(10, 8))\n",
    "    cs = ax.contour(mX, mY, f.reshape([ngrid, ngrid]), levels=7)\n",
    "    if Xsamples is not None:\n",
    "        ax.plot(Xsamples[:, 0], Xsamples[:, 1], 'r.')\n",
    "    ax.clabel(cs, inline=True, fontsize=8)\n",
    "    ax.grid()\n",
    "    ax.set_title(title)\n",
    "\n",
    "plot_contours(pX, X, title=\"True f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nX, nY= np.meshgrid(np.linspace(-7, 7, ngrid),\n",
    "                    np.linspace(-7, 7, ngrid))\n",
    "ngX = np.vstack([nX.flatten(), nY.flatten()]).T\n",
    "\n",
    "prX = np.exp(prior.log_prob(torch.tensor(ngX)).detach().numpy())\n",
    "pnX = mixture_px(ngX)\n",
    "\n",
    "\n",
    "nlevels = 20\n",
    "gmax = np.argmax(pnX)\n",
    "levels = np.linspace(0, pnX.max(), nlevels)\n",
    "thresh = [THRESH, pnX.max()]\n",
    "\n",
    "pXgY = np.array(prX)\n",
    "pXgY[pnX < THRESH] = -1\n",
    "xlevels = np.linspace(0, pXgY.max(), nlevels)\n",
    "\n",
    "_, ax = plt.subplots(dpi=150, figsize=(4, 4))\n",
    "ax.contourf(nX, nY, prX.reshape([ngrid, ngrid]), levels=nlevels,\n",
    "            cmap=\"Blues\")\n",
    "ax.set_axis_off()\n",
    "plt.show()\n",
    "\n",
    "_, ax = plt.subplots(dpi=150, figsize=(4, 4))\n",
    "ax.contourf(nX, nY, pnX.reshape([ngrid, ngrid]), levels=levels, cmap=\"Greys\")\n",
    "ax.plot(*ngX[gmax], 'x', color=\"white\", markersize=7, label=\"max\")\n",
    "ax.set_axis_off()\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams['hatch.color'] = \"white\"\n",
    "_, ax = plt.subplots(dpi=150, figsize=(4, 4))\n",
    "ax.contourf(nX, nY, pnX.reshape([ngrid, ngrid]), levels=levels, cmap=\"Greys\")\n",
    "ax.contour(nX, nY, pnX.reshape([ngrid, ngrid]), levels=thresh,\n",
    "              colors=\"white\")\n",
    "ax.contourf(nX, nY, pnX.reshape([ngrid, ngrid]), levels=thresh,\n",
    "               colors=\"white\", hatches=[\"///\", None], alpha=0.5)\n",
    "ax.set_axis_off()\n",
    "plt.show()\n",
    "\n",
    "_, ax = plt.subplots(dpi=150, figsize=(4, 4))\n",
    "ax.contour(nX, nY, pnX.reshape([ngrid, ngrid]), levels=thresh,\n",
    "              colors=\"white\")\n",
    "ax.contourf(nX, nY, pnX.reshape([ngrid, ngrid]), levels=levels,\n",
    "               cmap=\"Greys\")\n",
    "ax.contourf(nX, nY, pXgY.reshape([ngrid, ngrid]), levels=xlevels,\n",
    "               cmap=\"Blues\")\n",
    "ax.set_axis_off()\n",
    "plt.show()\n",
    "\n",
    "_, ax = plt.subplots(dpi=150, figsize=(4, 4))\n",
    "ax.contour(nX, nY, pnX.reshape([ngrid, ngrid]), levels=thresh,\n",
    "              colors=\"white\")\n",
    "ax.contourf(nX, nY, pnX.reshape([ngrid, ngrid]), levels=levels,\n",
    "               cmap=\"Greys\")\n",
    "ax.contourf(nX, nY, pnX.reshape([ngrid, ngrid]), levels=levels[levels >= THRESH],\n",
    "               cmap=\"Greens\", vmin=levels[0], vmax=levels[-1])\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimisation loop\n",
    "\n",
    "### Surrogate model initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "Xt = X\n",
    "Yt = torch.tensor(mixture_px(X)).float()\n",
    "\n",
    "# Testing\n",
    "Xs_np = mixture_sample(100)\n",
    "Xs = torch.tensor(Xs_np).float()\n",
    "Ys = torch.tensor(mixture_px(Xs_np)).float()\n",
    "\n",
    "\n",
    "# Adaptive threshold?\n",
    "print(f\"Initial threshold: {THRESH:.3f}\")\n",
    "if INCREASING_THRESH:\n",
    "    THRESH = AnnealedThreshold(PERCENTILE, TEMPERATURE)\n",
    "\n",
    "# Model initialisation\n",
    "if USE_CLASSIFIER:\n",
    "    model = ContinuousCPEModel(x_dim=2, latent_dim=100, dropoutp=0)\n",
    "    fit_cpe(\n",
    "        model,\n",
    "        Xt.float(), Yt.float(),\n",
    "        labeller=THRESH,\n",
    "        batch_size=len(Yt),\n",
    "        optimizer_options=dict(weight_decay=CLASSIFIER_REG),\n",
    "        stop_options=dict(n_window=2000)\n",
    "    )\n",
    "\n",
    "    for n, x, y in ((\"train\", Xt, Yt), (\"test\", Xs, Ys)):\n",
    "        z = (y > 0).detach().type(torch.int).flatten()\n",
    "        ez = (model(x) > np.log(.5)).type(torch.int).detach().flatten()\n",
    "        score = balanced_accuracy_score(ez, z)\n",
    "        print(f\"Clf {n} balanced acc. = {score:.3f}\")\n",
    "else:\n",
    "    kernel = ScaleKernel(MaternKernel(nu=1.5, lengthscale_prior=GammaPrior(3, 1)))\n",
    "    model = SingleTaskGP(train_X=Xt, train_Y=Yt.unsqueeze(-1),\n",
    "                         covar_module=kernel)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    mll = fit_gpytorch_mll(mll, max_attempts=10)\n",
    "\n",
    "    for n, x, y in ((\"train\", Xt, Yt), (\"test\", Xs, Ys)):\n",
    "        ey = np.asarray(model(x).loc.detach())\n",
    "        score = r2_score(ey, y.flatten().numpy())\n",
    "        print(f\"GP {n} R^2 = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLASSIFIER:\n",
    "    logPY = model(gXT).detach().numpy()\n",
    "    PY = np.exp(logPY)\n",
    "    _, axs = plt.subplots(1, 2, dpi=150, figsize=(10, 5))\n",
    "    plot_contours(PY, X, title=\"Initial classifier $p(y > \\\\tau | x)$\", ax=axs[0])\n",
    "    plot_contours(PY*(1-PY), X, title=\"Initial classifier variance\", ax=axs[1])\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    pred = model(gXT)\n",
    "    epX = pred.mean.detach()\n",
    "    spX = torch.sqrt(pred.variance).detach()\n",
    "\n",
    "    _, axs = plt.subplots(1, 2, dpi=150, figsize=(10, 5))\n",
    "    plot_contours(epX, X, title=\"Initial GP predictive mean\", ax=axs[0])\n",
    "    plot_contours(spX, X, title=\"Initial GP predictive confidence\", ax=axs[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = GaussianKDEProposal(2, k_components=10, scale=1., mu_scale_init=5)\n",
    "\n",
    "# Expected log likelihood\n",
    "if USE_CLASSIFIER:\n",
    "    acq = LogPIClassifierAcquisition(model=model)\n",
    "else:\n",
    "    acq = LogProbabilityOfImprovement(model=model, best_f=THRESH)\n",
    "\n",
    "# ELBO\n",
    "elbo = VariationalSearchAcquisition(acq, prior, kl_weight=1)\n",
    "\n",
    "iter, acquis, normgrad = [], [], []\n",
    "\n",
    "def callback(i: int, loss: torch.Tensor, grad: Tuple[torch.Tensor]):\n",
    "    \"\"\"For logging.\"\"\"\n",
    "    global iter\n",
    "    global acquis\n",
    "    global normgrad\n",
    "    iter.append(i)\n",
    "    acquis.append(-loss)\n",
    "    normgrad.append(np.mean([g.mean() for g in grad]))\n",
    "\n",
    "def plot_optimisation(iter, acquis, meangrad, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(dpi=150)\n",
    "    ax.plot(iter, meangrad)\n",
    "    ax.set_xlabel(\"Iterations\")\n",
    "    ax.set_ylabel(\"Mean Gradient\")\n",
    "    ax.grid()\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.set_ylabel(\"Acquisition\", color=\"tab:red\")\n",
    "    ax2.plot(iter, acquis, color=\"tab:red\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(1, N_ROUNDS+1):\n",
    "    print(f\"----- Round {r} -----\")\n",
    "\n",
    "    # Optimise search distribution\n",
    "    iter, acquis, normgrad = [], [], []\n",
    "    # Xc, cand_acquis = generate_candidates_reinforce( # If you want to use on-policy gradients\n",
    "    Xc, cand_acquis = generate_candidates_iw(\n",
    "        proposal_distribution=prop,\n",
    "        acquisition_function=elbo,\n",
    "        callback=callback,\n",
    "        stop_options=dict(maxiter=30000),\n",
    "        optimizer_options=dict(lr=1e-3),\n",
    "        gradient_samples=GRADIENT_SAMPLES,\n",
    "        candidate_samples=N_CANDIDATES\n",
    "    )\n",
    "\n",
    "    fig, axs = plt.subplots(3, 2, dpi=150, figsize=(10, 10))\n",
    "    plot_optimisation(iter, acquis, normgrad, axs[0, 0])\n",
    "\n",
    "    # Plot candidates on real density\n",
    "    plot_contours(pX, Xc, title=f\"R{r} q samples\", ax=axs[0, 1])\n",
    "\n",
    "    # Plot candidates on posterior density\n",
    "    logqX = prop.log_prob(gXT).detach()\n",
    "    qX = torch.exp(logqX)\n",
    "    plot_contours(qX, Xc, title=f\"R{r} q density\", ax=axs[2, 0])\n",
    "\n",
    "    # Update model and plot\n",
    "    Yc = torch.Tensor(mixture_px(Xc))\n",
    "    Xt = torch.concat((Xt, Xc))\n",
    "    Yt = torch.concat((Yt, Yc))\n",
    "    if INCREASING_THRESH:\n",
    "        THRESH(Yt)\n",
    "        print(f\"New threshold: {THRESH.best_f:.3f}\")\n",
    "    if USE_CLASSIFIER:\n",
    "        logPY = acq(gXT).detach().numpy()\n",
    "        PY = np.exp(logPY)\n",
    "        plot_contours(PY, Xc, title=f\"R{r} classifier $p(y > \\\\tau | x)$\",\n",
    "                      ax=axs[1, 0])\n",
    "        plot_contours(PY*(1-PY), Xc, title=f\"R{r} classifier variance\",\n",
    "                      ax=axs[1, 1])\n",
    "        fit_cpe(\n",
    "            model,\n",
    "            Xt.float(), Yt.float(),\n",
    "            labeller=THRESH,\n",
    "            batch_size=len(Yt),\n",
    "            optimizer_options=dict(weight_decay=CLASSIFIER_REG)\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        logPY = acq(gXT.unsqueeze(1)).detach().numpy()\n",
    "        pred = model(gXT)\n",
    "        spX = torch.sqrt(pred.variance).detach()\n",
    "        plot_contours(np.exp(logPY), Xc, title=f\"R{r} GP $p(y > \\\\tau | x)$\",\n",
    "                      ax=axs[1, 0])\n",
    "        plot_contours(spX, Xc, title=f\"R{r} GP predictive confidence\", ax=axs[1, 1])\n",
    "        model = model.condition_on_observations(Xc, Yc.unsqueeze(-1))\n",
    "        elbo.acq.model = model\n",
    "        elbo.acq.best_f = torch.tensor(THRESH)\n",
    "\n",
    "\n",
    "    # Plot candidates on acquisition function\n",
    "    logPYX = logPY + prior.log_prob(gXT).detach().numpy()\n",
    "    plot_contours(logPYX, Xc, title=f\"R{r} $\\\\log p(y > \\\\tau, x)$\",\n",
    "                  ax=axs[2, 1])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
